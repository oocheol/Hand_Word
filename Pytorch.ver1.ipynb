{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3bf568-679a-4516-9bc8-f14dcfb8c1c3",
   "metadata": {},
   "source": [
    "# 데이콘 BaseLine 위주 + 파이토치 도서(model) + 데이터 증강\n",
    "PUBLIC 점수 : 0.8785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "688d35d8-2479-4fe0-9afa-2657e63e899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.init\n",
    "import torchvision.datasets as datasets # 이미지 데이터셋 집합체\n",
    "import torchvision.transforms as transforms # 이미지 변환 툴\n",
    "from torch.utils.data import DataLoader # 학습 및 배치로 모델에 넣어주기 위한 툴\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim # 최적화 알고리즘들이 포함힘\n",
    "import torch.nn as nn # 신경망들이 포함됨\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b19514ce-0032-4c0d-8daa-62e32cf898d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') #GPU 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7933c4a-23d6-4ea5-bcab-f1039d128553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b0a88-880e-4542-8391-1ac0a6380f63",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BaseLine과의 차이점\n",
    "CFG에서 IMG_SIZE를 128에서 32로 줄임, LEARNING_RAGE를 1e-3으로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "211cd835-c327-4e18-9096-d37aa04ec585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터 튜닝\n",
    "\n",
    "CFG = {\n",
    "    'IMG_SIZE':32, # baseline 이미지 사이즈 128\n",
    "    'EPOCHS':50, #에포크\n",
    "    'LEARNING_RATE':1e-3, #학습률\n",
    "    'BATCH_SIZE':12, #배치사이즈 # 처음엔12\n",
    "    'SEED':41, #시드\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a30a569-d188-4661-9b7f-c98dd8619dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8d13005-2afb-4442-92aa-b3c61155064a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.png</td>\n",
       "      <td>10-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.png</td>\n",
       "      <td>10-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.png</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name label\n",
       "0   001.png  10-2\n",
       "1   002.png  10-1\n",
       "2   003.png     3\n",
       "3   004.png     8\n",
       "4   005.png     9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "label_df = pd.read_csv('user_data/train.csv')\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2e823eb-bd5b-4705-81de-ed0154276114",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df['label'][label_df['label'] == '10-1'] = 10 ## label : 10-1 -> 10\n",
    "label_df['label'][label_df['label'] == '10-2'] = 0 ## Label : 10-2 -> 0\n",
    "label_df['label'] = label_df['label'].apply(lambda x : int(x)) ## Dtype : object -> int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac604f0-fe99-4a31-bd57-da55b90321af",
   "metadata": {},
   "source": [
    "# BaseLine과의 차이점\n",
    "파일 경로 에러나서 re모듈 이용하여 \\\\ -> /로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5185ed96-9346-4110-b6a7-9870ce1f4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(data_dir):\n",
    "    img_path_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    # get image path\n",
    "    img_path_list.extend([re.sub('\\\\\\\\','/',i) for i in glob(os.path.join(data_dir, '*.png'))])\n",
    "    img_path_list.sort(key=lambda x:int(x.split('/')[-1].split('.')[0]))\n",
    "        \n",
    "    # get label\n",
    "    #label_df = pd.read_csv(data_dir+'/train.csv')\n",
    "    label_list.extend(label_df['label'])\n",
    "                \n",
    "    return img_path_list, label_list\n",
    "\n",
    "def get_test_data(data_dir):\n",
    "    img_path_list = []\n",
    "    \n",
    "    # get image path\n",
    "    img_path_list.extend([re.sub('\\\\\\\\','/',i) for i in glob(os.path.join(data_dir, '*.png'))])\n",
    "    img_path_list.sort(key=lambda x:int(x.split('/')[-1].split('.')[0]))\n",
    "    \n",
    "    return img_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4960d5bc-f0f4-496e-a6be-73f93218b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_path, all_label = get_train_data('user_data/train')\n",
    "test_img_path = get_test_data('user_data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb73c1-e299-4b02-a147-4e87c8461a62",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfec1127-44e9-4ca1-a61c-b99f030ed2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, train_mode=True, transforms=None): #필요한 변수들을 선언\n",
    "        self.transforms = transforms\n",
    "        self.train_mode = train_mode\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, index): #index번째 data를 return\n",
    "        img_path = self.img_path_list[index]\n",
    "        # Get image data\n",
    "        image = cv2.imread(img_path)\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        if self.train_mode:\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "        \n",
    "    \n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    def __len__(self): #길이 return\n",
    "        return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae8c0de6-ef03-4dba-8a38-b8cdf96f09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train : Validation = 0.8 : 0.25 Split\n",
    "train_len = int(len(all_img_path)*0.75)\n",
    "Vali_len = int(len(all_img_path)*0.25)\n",
    "\n",
    "train_img_path = all_img_path[:train_len]\n",
    "train_label = all_label[:train_len]\n",
    "\n",
    "vali_img_path = all_img_path[train_len:]\n",
    "vali_label = all_label[train_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a5ca72-ba5b-41fd-a46c-0ef04718246d",
   "metadata": {},
   "source": [
    "# BaseLine과의 차이점\n",
    "RandomHorizontalFlip(좌우변경), RandomRotation(각도 랜덤), RandomPerspective(원근법?) 추가하여 데이터 증강때 사용  \n",
    "제출당시 RandomRotation, RandomPerspective 하이퍼파라미터 정확히 기억은 안나지만 (10, 0.1 or 15,0.15) 둘중 하나 였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bdb4c22-0dc2-4c92-ab66-6876fccd84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(), #Numpy배열에서 PIL이미지로\n",
    "                    transforms.Resize([CFG['IMG_SIZE'], CFG['IMG_SIZE']]), #이미지 사이즈 변형\n",
    "                    transforms.RandomHorizontalFlip(), # Horizontal Flip\n",
    "                    # transforms.RandomCrop(32, padding=2), # Centre Crop 이거 하면 오히려 점수 잘 안나옴\n",
    "                    transforms.RandomRotation(degrees=10,interpolation=transforms.InterpolationMode.NEAREST),\n",
    "                    transforms.RandomPerspective(distortion_scale=.15,p=.15,interpolation=transforms.InterpolationMode.NEAREST),\n",
    "                    transforms.ToTensor(), #이미지 데이터를 tensor\n",
    "                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) #이미지 정규화\n",
    "                    \n",
    "                    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize([CFG['IMG_SIZE'], CFG['IMG_SIZE']]),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bace2c8-d25b-4153-89fa-8dd2c9ffdae0",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f1eb53-51c3-485f-bacd-eb433225ad1f",
   "metadata": {},
   "source": [
    "# BaseLine과의 차이점\n",
    "데이터 증강 이렇게 하는지는 잘 모르겠지만 total train batches 개수 보니까 데이터 수 증가된것으로 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f559060-66d3-4875-baba-af77cbf5a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_img_path, train_label, train_mode=True, transforms=train_transform) ## 맨 처음 선언 안하면 오류뜸\n",
    "vali_dataset = CustomDataset(vali_img_path, vali_label, train_mode=True, transforms=test_transform)\n",
    "\n",
    "for i in range(10): ## 데이터 수 10배 증가 (100배로 증가시키고나서 진행해봤지만 성능은 더 안좋아졌습니다. 0.78정도)\n",
    "    train_dataset+=CustomDataset(train_img_path, train_label, train_mode=True, transforms=train_transform)\n",
    "    vali_dataset += CustomDataset(vali_img_path, vali_label, train_mode=True, transforms=test_transform)\n",
    "    \n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "vali_loader = DataLoader(vali_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8ad207e-5af1-460a-8f4d-65d4e43085d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train imgs : 643 / total train batches : 590\n",
      "total valid imgs : 214 / total valid batches : 198\n"
     ]
    }
   ],
   "source": [
    "train_batches = len(train_loader)\n",
    "vali_batches = len(vali_loader)\n",
    "\n",
    "print('total train imgs :',train_len,'/ total train batches :', train_batches) ## 643개의 이미지 이용, 총 배치 개수 54 -> 643개의 이미지 이용, 총 배치 개수 590\n",
    "print('total valid imgs :',Vali_len, '/ total valid batches :', vali_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c9ca44-1654-436c-b557-d0428604493a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 이미지 및 shape 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd141720-6e76-4083-81dd-167e84c2a65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_features, train_labels = next(iter(train_loader)) #iter는 반복 가능한 객체에서 이터레이터를 반환하고, \n",
    "                                                        #next는 이터레이터에서 값을 차례대로 꺼냅니다. \n",
    "img = train_features[0]\n",
    "label = train_labels[0]\n",
    "plt.imshow(img[0], cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6a266-cea0-451f-aaac-f9b43e158cdc",
   "metadata": {},
   "source": [
    "# BaseLine과의 차이점\n",
    "모델 조금 수정함, pytorch 관련 도서 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e4caae6-573a-4730-8e23-d6c57dfc35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNclassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNclassification, self).__init__()\n",
    "        self.keep_prob = 0.5 ## dropout에서 쓰임\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            ##채널=1\n",
    "            torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)) ## 절반으로 줄어듬\n",
    "        \n",
    "        \n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        \n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(4 * 4 * 128, 363, bias=True)\n",
    "        # torch.nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            self.fc1,\n",
    "            torch.nn.ReLU(), ##ReLU Sigmoid\n",
    "            torch.nn.Dropout(p=1 - self.keep_prob))\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(363, 11, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.layer4(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b6a39d2-682f-47fd-83d6-c0518e7be86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 정의\n",
    "model = CNNclassification().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46196db5-50f9-475f-93db-429e6d9f8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)    # 비용 함수에 소프트맥스 함수 포함되어져 있음.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CFG[\"LEARNING_RATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2157b570-00d0-4a2d-9c4b-be5c24725bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNclassification().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234db4ef-c56d-47fd-b1eb-ff665f651ef4",
   "metadata": {},
   "source": [
    "# BaseLine과의 차이점\n",
    "tqdm부분만 제거 (성능에 영향X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c97b9f46-48c0-4c18-9dc1-13cf35bc96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, scheduler, device): \n",
    "    model.to(device)\n",
    "    n = len(train_loader)\n",
    "    \n",
    "    #Loss Function 정의\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(1,CFG[\"EPOCHS\"]+1): #에포크 설정\n",
    "        model.train() #모델 학습\n",
    "        running_loss = 0.0\n",
    "            \n",
    "        for img, label in iter(train_loader): # tqdm\n",
    "            img, label = img.to(device), label.to(device) #배치 데이터\n",
    "            optimizer.zero_grad() #배치마다 optimizer 초기화\n",
    "        \n",
    "            # Data -> Model -> Output\n",
    "            logit = model(img) #예측값 산출\n",
    "            loss = criterion(logit, label) #손실함수 계산\n",
    "            \n",
    "            # 역전파\n",
    "            loss.backward() #손실함수 기준 역전파 \n",
    "            optimizer.step() #가중치 최적화\n",
    "            running_loss += loss.item()\n",
    "              \n",
    "        print('[%d] Train loss: %.10f' %(epoch, running_loss / len(train_loader)))\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        #Validation set 평가\n",
    "        model.eval() #evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수\n",
    "        vali_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad(): #파라미터 업데이트 안하기 때문에 no_grad 사용\n",
    "            for img, label in iter(vali_loader): # tqdm\n",
    "                img, label = img.to(device), label.to(device)\n",
    "\n",
    "                logit = model(img)\n",
    "                vali_loss += criterion(logit, label)\n",
    "                pred = logit.argmax(dim=1, keepdim=True)  #11개의 class중 가장 값이 높은 것을 예측 label로 추출\n",
    "                correct += pred.eq(label.view_as(pred)).sum().item() #예측값과 실제값이 맞으면 1 아니면 0으로 합산\n",
    "        vali_acc = 100 * correct / len(vali_loader.dataset)\n",
    "        print('Vail set: Loss: {:.4f}, Accuracy: {}/{} ( {:.0f}%)\\n'.format(vali_loss / len(vali_loader), correct, len(vali_loader.dataset), 100 * correct / len(vali_loader.dataset))) \n",
    "        \n",
    "        #베스트 모델 저장\n",
    "        if best_acc < vali_acc:\n",
    "            best_acc = vali_acc\n",
    "            torch.save(model.state_dict(), './saved/best_model.pth') #이 디렉토리에 best_model.pth을 저장\n",
    "            # print('Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cf65157-82b3-469f-add1-5677718c8c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train loss: 2.3907265291\n",
      "Vail set: Loss: 2.0106, Accuracy: 517/2365 ( 22%)\n",
      "\n",
      "[2] Train loss: 1.8780648547\n",
      "Vail set: Loss: 1.6524, Accuracy: 781/2365 ( 33%)\n",
      "\n",
      "[3] Train loss: 1.6771081167\n",
      "Vail set: Loss: 1.4683, Accuracy: 869/2365 ( 37%)\n",
      "\n",
      "[4] Train loss: 1.5520667015\n",
      "Vail set: Loss: 1.3219, Accuracy: 1056/2365 ( 45%)\n",
      "\n",
      "[5] Train loss: 1.4576763640\n",
      "Vail set: Loss: 1.2042, Accuracy: 1287/2365 ( 54%)\n",
      "\n",
      "[6] Train loss: 1.3388714004\n",
      "Vail set: Loss: 1.0859, Accuracy: 1397/2365 ( 59%)\n",
      "\n",
      "[7] Train loss: 1.2803874508\n",
      "Vail set: Loss: 0.9395, Accuracy: 1573/2365 ( 67%)\n",
      "\n",
      "[8] Train loss: 1.2113900319\n",
      "Vail set: Loss: 1.0306, Accuracy: 1507/2365 ( 64%)\n",
      "\n",
      "[9] Train loss: 1.1688589824\n",
      "Vail set: Loss: 0.9304, Accuracy: 1639/2365 ( 69%)\n",
      "\n",
      "[10] Train loss: 1.1289709547\n",
      "Vail set: Loss: 1.0247, Accuracy: 1441/2365 ( 61%)\n",
      "\n",
      "[11] Train loss: 1.1043160557\n",
      "Vail set: Loss: 0.9215, Accuracy: 1595/2365 ( 67%)\n",
      "\n",
      "[12] Train loss: 1.0646186306\n",
      "Vail set: Loss: 0.8514, Accuracy: 1716/2365 ( 73%)\n",
      "\n",
      "[13] Train loss: 1.0495735336\n",
      "Vail set: Loss: 0.8759, Accuracy: 1628/2365 ( 69%)\n",
      "\n",
      "[14] Train loss: 1.0406334473\n",
      "Vail set: Loss: 0.8438, Accuracy: 1639/2365 ( 69%)\n",
      "\n",
      "[15] Train loss: 1.0185742329\n",
      "Vail set: Loss: 0.7541, Accuracy: 1771/2365 ( 75%)\n",
      "\n",
      "[16] Train loss: 0.9750810689\n",
      "Vail set: Loss: 0.8481, Accuracy: 1672/2365 ( 71%)\n",
      "\n",
      "[17] Train loss: 0.9791256257\n",
      "Vail set: Loss: 0.7635, Accuracy: 1793/2365 ( 76%)\n",
      "\n",
      "[18] Train loss: 0.9056315407\n",
      "Vail set: Loss: 0.7929, Accuracy: 1782/2365 ( 75%)\n",
      "\n",
      "[19] Train loss: 0.8515655365\n",
      "Vail set: Loss: 0.7983, Accuracy: 1771/2365 ( 75%)\n",
      "\n",
      "[20] Train loss: 0.8309298746\n",
      "Vail set: Loss: 0.7821, Accuracy: 1782/2365 ( 75%)\n",
      "\n",
      "[21] Train loss: 0.7821272762\n",
      "Vail set: Loss: 0.6702, Accuracy: 1881/2365 ( 80%)\n",
      "\n",
      "[22] Train loss: 0.7549531579\n",
      "Vail set: Loss: 0.6428, Accuracy: 1881/2365 ( 80%)\n",
      "\n",
      "[23] Train loss: 0.7168703629\n",
      "Vail set: Loss: 0.6436, Accuracy: 1881/2365 ( 80%)\n",
      "\n",
      "[24] Train loss: 0.6907030573\n",
      "Vail set: Loss: 0.5955, Accuracy: 1826/2365 ( 77%)\n",
      "\n",
      "[25] Train loss: 0.6850511161\n",
      "Vail set: Loss: 0.6158, Accuracy: 1980/2365 ( 84%)\n",
      "\n",
      "[26] Train loss: 0.6780483916\n",
      "Vail set: Loss: 0.5797, Accuracy: 1914/2365 ( 81%)\n",
      "\n",
      "[27] Train loss: 0.6668470570\n",
      "Vail set: Loss: 0.6455, Accuracy: 1892/2365 ( 80%)\n",
      "\n",
      "[28] Train loss: 0.6346507215\n",
      "Vail set: Loss: 0.6761, Accuracy: 1914/2365 ( 81%)\n",
      "\n",
      "[29] Train loss: 0.6238891579\n",
      "Vail set: Loss: 0.5750, Accuracy: 1991/2365 ( 84%)\n",
      "\n",
      "[30] Train loss: 0.5537450582\n",
      "Vail set: Loss: 0.6241, Accuracy: 1936/2365 ( 82%)\n",
      "\n",
      "[31] Train loss: 0.5173679636\n",
      "Vail set: Loss: 0.5956, Accuracy: 1914/2365 ( 81%)\n",
      "\n",
      "[32] Train loss: 0.4938533894\n",
      "Vail set: Loss: 0.6095, Accuracy: 1870/2365 ( 79%)\n",
      "\n",
      "[33] Train loss: 0.4733316288\n",
      "Vail set: Loss: 0.5932, Accuracy: 1936/2365 ( 82%)\n",
      "\n",
      "[34] Train loss: 0.4394227938\n",
      "Vail set: Loss: 0.6285, Accuracy: 1936/2365 ( 82%)\n",
      "\n",
      "[35] Train loss: 0.4302046819\n",
      "Vail set: Loss: 0.5687, Accuracy: 1947/2365 ( 82%)\n",
      "\n",
      "[36] Train loss: 0.3797523805\n",
      "Vail set: Loss: 0.6047, Accuracy: 1991/2365 ( 84%)\n",
      "\n",
      "[37] Train loss: 0.3690503722\n",
      "Vail set: Loss: 0.6213, Accuracy: 1936/2365 ( 82%)\n",
      "\n",
      "[38] Train loss: 0.3614360230\n",
      "Vail set: Loss: 0.5766, Accuracy: 2002/2365 ( 85%)\n",
      "\n",
      "[39] Train loss: 0.3350084461\n",
      "Vail set: Loss: 0.5424, Accuracy: 2024/2365 ( 86%)\n",
      "\n",
      "[40] Train loss: 0.3149622545\n",
      "Vail set: Loss: 0.8625, Accuracy: 1936/2365 ( 82%)\n",
      "\n",
      "[41] Train loss: 0.3151686326\n",
      "Vail set: Loss: 0.6188, Accuracy: 1991/2365 ( 84%)\n",
      "\n",
      "[42] Train loss: 0.2839863678\n",
      "Vail set: Loss: 0.6788, Accuracy: 1980/2365 ( 84%)\n",
      "\n",
      "[43] Train loss: 0.2886294005\n",
      "Vail set: Loss: 0.6763, Accuracy: 2002/2365 ( 85%)\n",
      "\n",
      "[44] Train loss: 0.2508264488\n",
      "Vail set: Loss: 0.7396, Accuracy: 1991/2365 ( 84%)\n",
      "\n",
      "[45] Train loss: 0.2351061377\n",
      "Vail set: Loss: 0.7608, Accuracy: 1947/2365 ( 82%)\n",
      "\n",
      "[46] Train loss: 0.2288411033\n",
      "Vail set: Loss: 0.6709, Accuracy: 1980/2365 ( 84%)\n",
      "\n",
      "[47] Train loss: 0.2217088692\n",
      "Vail set: Loss: 0.5741, Accuracy: 2057/2365 ( 87%)\n",
      "\n",
      "[48] Train loss: 0.2275164340\n",
      "Vail set: Loss: 0.6966, Accuracy: 2035/2365 ( 86%)\n",
      "\n",
      "[49] Train loss: 0.2082910396\n",
      "Vail set: Loss: 0.5984, Accuracy: 2013/2365 ( 85%)\n",
      "\n",
      "[50] Train loss: 0.1962720258\n",
      "Vail set: Loss: 0.6367, Accuracy: 2013/2365 ( 85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b382ac40-f61c-45e0-8307-df199861f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    model_pred = []\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(iter(test_loader)):\n",
    "            img = img.to(device)\n",
    "\n",
    "            pred_logit = model(img)\n",
    "            pred_logit = pred_logit.argmax(dim=1, keepdim=True).squeeze(1)\n",
    "\n",
    "            model_pred.extend(pred_logit.tolist())\n",
    "    return model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be70ce47-9f4c-4674-a824-e833b923ee4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09e42fe763e4324a195e8f225e5e3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 6, 8]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = CustomDataset(test_img_path, None, train_mode=False, transforms=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "# Validation Accuracy가 가장 뛰어난 모델을 불러옵니다.\n",
    "checkpoint = torch.load('./saved/best_model.pth')\n",
    "model = CNNclassification().to(device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Inference\n",
    "preds = predict(model, test_loader, device)\n",
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "106ab577-2e6e-4bdf-912f-e55b0e210226",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('user_data/sample_submission.csv')\n",
    "submission['label'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21dfd95b-a572-446b-bffe-e15d06dab128",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label'][submission['label'] == 10] = '10-1' ## label : 10 -> '10-1'\n",
    "submission['label'][submission['label'] == 0] = '10-2' ## Label : 0 -> '10-2'\n",
    "submission['label'] = submission['label'].apply(lambda x : str(x)) ## Dtype : int -> object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3508094-290e-4d9f-8234-9d62fc92f0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.png</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.png</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>006.png</td>\n",
       "      <td>10-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>007.png</td>\n",
       "      <td>10-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>008.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>009.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>010.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name label\n",
       "0   001.png     1\n",
       "1   002.png     2\n",
       "2   003.png     1\n",
       "3   004.png     6\n",
       "4   005.png     8\n",
       "5   006.png  10-1\n",
       "6   007.png  10-1\n",
       "7   008.png     2\n",
       "8   009.png     4\n",
       "9   010.png     4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a51a1a81-7182-4d03-a7ef-c652db3ee10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./제출/submit_TAVE_MJei_code.csv', index=False) ## -> PUBLIC 점수 0.8785"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
